{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7568bd02",
   "metadata": {},
   "source": [
    "# Current status of the top model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803ea726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install pyg -c pyg\n",
    "# !pip install -U git+https://github.com/jcmgray/cotengra.git\n",
    "# !pip install opt-einsum\n",
    "# !conda install matplotlib\n",
    "# !pip install pyzx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9521680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\pitog\\lib\\site-packages\\cotengra\\hyper.py:29: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\"Couldn't import `kahypar` - skipping from default \"\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyzx as zx\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "from cotengra import ContractionTree\n",
    "from opt_einsum import RandomGreedy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from caramel.models.circuits_to_dataset.enhance_data_set_builder import CircuitDataset as DualCircuitDataset\n",
    "from caramel.interface_pyzx import Network\n",
    "from caramel.utils import  edge_path_to_opt_einsum_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be8aa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Device set!-\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seed\n",
    "random.seed(1997)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\n-Device set!-\\n\\n\")\n",
    "\n",
    "# Get data set\n",
    "tf = ['transpile_graphene-x1-y1-steps4.qasm', 'tof_10_after_heavy', 'tof_10_after_light', 'tof_10_before',\n",
    "      'tof_10_pyzx.qc', 'tof_10_tpar.qc', 'tof_3_after_heavy', 'tof_3_after_light',\n",
    "      'tof_3_before', 'tof_3_pyzx.qc', 'tof_3_tpar.qc', 'tof_4_after_heavy', 'tof_4_after_light',\n",
    "      'tof_4_before', 'tof_4_pyzx.qc', 'tof_4_tpar.qc', 'tof_5_after_heavy', 'tof_5_after_light',\n",
    "      'tof_5_before', 'tof_5_pyzx.qc', ]\n",
    "#'000_test_circuit.qasm'\n",
    "# ZX circuit path\n",
    "zx_folder_path = \"circuit_dataset/zx_circuits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f383a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caramel\\caramel\\cost_function.py\n",
    "\n",
    "# caramel\\caramel\\cost_function.py\n",
    "\n",
    "def cost_of_contraction(path, quantum_network, importance):\n",
    "    \"\"\"\n",
    "    :path:\n",
    "    :quantum_net:\n",
    "    :importance: [ a, b, c] = [sum of the flops cont by every nod ein the tree,\n",
    "                                log 2 of the size of the largest tensor,\n",
    "                                total amount of created  memory]\n",
    "    :return: total cost of contraction\n",
    "    \"\"\"\n",
    "\n",
    "    tree = ContractionTree.from_path(inputs=quantum_network[\"opt_einsum_input\"],\n",
    "                                     output=quantum_network[\"opt_einsum_output\"],\n",
    "                                     size_dict=quantum_network[\"size_dict\"],\n",
    "                                     path=path)\n",
    "    #     print(\n",
    "    #         \"log10[FLOPs]: \",\n",
    "    #         \"%.3f\" % np.log10(float(tree.total_flops()+0.01)),  # sum of the flops cont by every nod ein the tree\n",
    "    #         \" log2[SIZE]: \",\n",
    "    #         \"%.0f\" % tree.contraction_width(),  # log 2 of the size of the largest tensor\n",
    "    #         \" log2[WRITE]: \",\n",
    "    #         \"%.3f\" % np.log2(float(tree.total_write()+0.1)),  # total amount of created  memory\n",
    "\n",
    "    cost = importance[0] * np.log10(float(tree.total_flops()) + 0.1) + importance[1] * np.log2(\n",
    "        tree.contraction_width() + 1.0) + importance[\n",
    "               2] * np.log2(float(tree.total_write() + 0.1))\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5e1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference heuristic\n",
    "def get_reference_performance(quantum_net, importance):\n",
    "    optimizer = RandomGreedy()\n",
    "\n",
    "    contraction_path = optimizer(inputs=quantum_net.opt_einsum_input.copy(),\n",
    "                                 output=quantum_net.opt_einsum_output.copy(),\n",
    "                                 size_dict=quantum_net.size_dict.copy(),\n",
    "                                 memory_limit=500)\n",
    "    quantum_net_as_dic = {\"opt_einsum_input\": quantum_net.opt_einsum_input,\n",
    "                          \"opt_einsum_output\": quantum_net.opt_einsum_output,\n",
    "                          \"size_dict\": quantum_net.size_dict}\n",
    "    contraction_cost = cost_of_contraction(contraction_path, quantum_net_as_dic, importance)\n",
    "\n",
    "    return contraction_cost, contraction_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0dea79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the performance of some random contractions:\n",
    "\n",
    "def circuit_stats(circuit_name, nr_samples, importance):\n",
    "    np.random.permutation(10)\n",
    "    circuit_path = zx_folder_path + circuit_name\n",
    "    tensor_circuit = zx.Circuit.load(circuit_path)\n",
    "    zx_graph = tensor_circuit.to_graph()\n",
    "    quantum_net = Network(zx_graph)\n",
    "    nr_edges = len(quantum_net.size_dict.keys())\n",
    "    reference_cost, reference_path = get_reference_performance(quantum_net, importance)\n",
    "\n",
    "    random_contraction_costs = []\n",
    "    for i in range(nr_samples):\n",
    "        # Random path\n",
    "        contraction_path = np.random.permutation(nr_edges)\n",
    "        contraction_path = edge_path_to_opt_einsum_path(contraction_path, quantum_net.opt_einsum_input.copy())\n",
    "\n",
    "        quantum_net_as_dic = {\"opt_einsum_input\": quantum_net.opt_einsum_input,\n",
    "                              \"opt_einsum_output\": quantum_net.opt_einsum_output,\n",
    "                              \"size_dict\": quantum_net.size_dict}\n",
    "        cost = cost_of_contraction(contraction_path, quantum_net_as_dic, importance)\n",
    "        random_contraction_costs.append(cost)\n",
    "\n",
    "    return random_contraction_costs, reference_cost, reference_path\n",
    "\n",
    "\n",
    "# The histogram of the contraction data\n",
    "def circuit_histogram(circuit_name, contraction_costs, reference_cost):\n",
    "    fig = plt.figure()\n",
    "    n, bins, patches = plt.hist(contraction_costs, facecolor='g', alpha=0.75)\n",
    "    plt.axvline(x=reference_cost, color='b', label='reference')\n",
    "    mean_cost = 0\n",
    "    for c in contraction_costs:\n",
    "        mean_cost = mean_cost + c\n",
    "    mean_cost = mean_cost / len(contraction_costs)\n",
    "    plt.axvline(x=mean_cost, color='r', label='mean')\n",
    "    plt.xlabel('Cost')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Histogram ' + circuit_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8205af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vals: ['pi', 'pi/4', '-pi']\n",
      "phases: [Fraction(1, 1), Fraction(1, 4), Fraction(1, 1)]\n",
      "vals: ['pi', 'pi/4', '-pi']\n",
      "phases: [Fraction(1, 1), Fraction(1, 4), Fraction(1, 1)]\n",
      "vals: ['pi', 'pi/4', '-pi']\n",
      "phases: [Fraction(1, 1), Fraction(1, 4), Fraction(1, 1)]\n",
      "vals: ['pi', 'pi/4', '-pi']\n",
      "phases: [Fraction(1, 1), Fraction(1, 4), Fraction(1, 1)]\n",
      "vals: ['pi', 'pi/4', '-pi']\n",
      "phases: [Fraction(1, 1), Fraction(1, 4), Fraction(1, 1)]\n",
      "vals: ['pi', 'pi/4', '-pi']\n",
      "phases: [Fraction(1, 1), Fraction(1, 4), Fraction(1, 1)]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m circuits_histograms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m tf:\n\u001b[1;32m----> 9\u001b[0m     contraction_costs_random, cost_reference, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_paths_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_paths_samples):\n\u001b[0;32m     11\u001b[0m         contraction_costs_all[i] \u001b[38;5;241m=\u001b[39m contraction_costs_all[i] \u001b[38;5;241m+\u001b[39m contraction_costs_random[i]\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mcircuit_stats\u001b[1;34m(circuit_name, nr_samples, importance)\u001b[0m\n\u001b[0;32m     16\u001b[0m     contraction_path \u001b[38;5;241m=\u001b[39m edge_path_to_opt_einsum_path(contraction_path, quantum_net\u001b[38;5;241m.\u001b[39mopt_einsum_input\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m     18\u001b[0m     quantum_net_as_dic \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt_einsum_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: quantum_net\u001b[38;5;241m.\u001b[39mopt_einsum_input,\n\u001b[0;32m     19\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt_einsum_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: quantum_net\u001b[38;5;241m.\u001b[39mopt_einsum_output,\n\u001b[0;32m     20\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: quantum_net\u001b[38;5;241m.\u001b[39msize_dict}\n\u001b[1;32m---> 21\u001b[0m     cost \u001b[38;5;241m=\u001b[39m \u001b[43mcost_of_contraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontraction_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantum_net_as_dic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     random_contraction_costs\u001b[38;5;241m.\u001b[39mappend(cost)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m random_contraction_costs, reference_cost, reference_path\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mcost_of_contraction\u001b[1;34m(path, quantum_network, importance)\u001b[0m\n\u001b[0;32m     15\u001b[0m tree \u001b[38;5;241m=\u001b[39m ContractionTree\u001b[38;5;241m.\u001b[39mfrom_path(inputs\u001b[38;5;241m=\u001b[39mquantum_network[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt_einsum_input\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     16\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mquantum_network[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt_einsum_output\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     17\u001b[0m                                  size_dict\u001b[38;5;241m=\u001b[39mquantum_network[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m                                  path\u001b[38;5;241m=\u001b[39mpath)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     print(\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#         \"log10[FLOPs]: \",\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#         \"%.3f\" % np.log10(float(tree.total_flops()+0.01)),  # sum of the flops cont by every nod ein the tree\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#         \" log2[WRITE]: \",\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#         \"%.3f\" % np.log2(float(tree.total_write()+0.1)),  # total amount of created  memory\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m cost \u001b[38;5;241m=\u001b[39m importance[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(\u001b[38;5;28mfloat\u001b[39m(\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_flops\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;241m+\u001b[39m importance[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(\n\u001b[0;32m     28\u001b[0m     tree\u001b[38;5;241m.\u001b[39mcontraction_width() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m+\u001b[39m importance[\n\u001b[0;32m     29\u001b[0m            \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(\u001b[38;5;28mfloat\u001b[39m(tree\u001b[38;5;241m.\u001b[39mtotal_write() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cost\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pitog\\lib\\site-packages\\cotengra\\core.py:532\u001b[0m, in \u001b[0;36mContractionTree.total_flops\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flops \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverse():\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flops \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_flops(node)\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_track_flops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pitog\\lib\\site-packages\\cotengra\\core.py:885\u001b[0m, in \u001b[0;36mContractionTree.traverse\u001b[1;34m(self, order)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m queue:\n\u001b[0;32m    884\u001b[0m     node \u001b[38;5;241m=\u001b[39m queue[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 885\u001b[0m     l, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;66;03m# both node's children are ready -> we can yield this contraction\u001b[39;00m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (l \u001b[38;5;129;01min\u001b[39;00m ready) \u001b[38;5;129;01mand\u001b[39;00m (r \u001b[38;5;129;01min\u001b[39;00m ready):\n",
      "\u001b[1;31mKeyError\u001b[0m: frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39})"
     ]
    }
   ],
   "source": [
    "# Kind of a random heuristic:\n",
    "nr_paths_samples = 100\n",
    "importance_weights = [1, 0.5, 0.0]\n",
    "\n",
    "contraction_costs_all = [0 for i in range(nr_paths_samples)]\n",
    "reference_cost_all = 0\n",
    "circuits_histograms = []\n",
    "\n",
    "for name in tf:\n",
    "    print(\"name:\",name)\n",
    "    contraction_costs_random, cost_reference, _ = circuit_stats(name, nr_paths_samples, importance_weights)\n",
    "    for i in range(nr_paths_samples):\n",
    "        contraction_costs_all[i] = contraction_costs_all[i] + contraction_costs_random[i]\n",
    "    reference_cost_all = reference_cost_all + cost_reference\n",
    "    circuits_histograms.append(circuit_histogram(name, contraction_costs_random, cost_reference))\n",
    "\n",
    "print(\"reference value:\", reference_cost_all)\n",
    "histogram_all = circuit_histogram(\"performance of a random_heuristic\", contraction_costs_all, reference_cost_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b18afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caramel\\models\\circuits_to_dataset\\enhance_data_set_builder.py\n",
    "# Create the pytorch data set\n",
    "dataset = DualCircuitDataset(root='circuit_dataset/dual_experiment_dataset/',\n",
    "                             target_files=tf)\n",
    "\n",
    "# Node features [nr_nodes,3]\n",
    "# order  -> -1 for edges that are not and edges\n",
    "#           or 'i' where 'i' is the position in the output_order\n",
    "# node1 rank -> tensor 1 size\n",
    "# node2 rank -> tensor 2 size\n",
    "\n",
    "# Edge features [nr_edges, 2]\n",
    "# node id\n",
    "# node size\n",
    "\n",
    "\n",
    "print(\"dataset:\", dataset)\n",
    "print(\"\\n-Data extracted!- \\n\\n\")\n",
    "\n",
    "data = dataset[0]\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330843fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d427ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caramel\\example_contraction_learning.py\n",
    "# Model\n",
    "\n",
    "class Model01(torch.nn.Module):\n",
    "    def __init__(self, feature_size, model_params):\n",
    "        super(Model01, self).__init__()\n",
    "\n",
    "        ########################################################################\n",
    "        ##   How to change the code bellow in order to have a good model ?    ##\n",
    "        ########################################################################\n",
    "        embedding_size = model_params[\"embedding_size\"]\n",
    "        n_heads = model_params[\"n_heads\"]\n",
    "        dropout_rate = model_params[\"dropout_rate\"]\n",
    "        edge_dim = model_params[\"edge_dim\"]\n",
    "        self.n_layers = model_params[\"model_layers\"]\n",
    "\n",
    "        # Transformation layer\n",
    "        self.conv1 = TransformerConv(feature_size,\n",
    "                                     embedding_size,\n",
    "                                     heads=n_heads,\n",
    "                                     dropout=dropout_rate,\n",
    "                                     edge_dim=edge_dim,\n",
    "                                     beta=True)\n",
    "        self.transf1 = Linear(embedding_size * n_heads, embedding_size)\n",
    "        self.bn1 = BatchNorm1d(embedding_size)\n",
    "\n",
    "        # middle\n",
    "        self.conv_layers = ModuleList([])\n",
    "        self.transf_layers = ModuleList([])\n",
    "        for i in range(self.n_layers):\n",
    "            self.conv_layers.append(TransformerConv(embedding_size,\n",
    "                                                    embedding_size,\n",
    "                                                    heads=n_heads,\n",
    "                                                    dropout=dropout_rate,\n",
    "                                                    edge_dim=edge_dim,\n",
    "                                                    beta=True))\n",
    "            self.transf_layers.append(Linear(embedding_size * n_heads, embedding_size))\n",
    "\n",
    "        self.conv2 = TransformerConv(embedding_size,\n",
    "                                     1,\n",
    "                                     heads=n_heads,\n",
    "                                     dropout=dropout_rate,\n",
    "                                     edge_dim=edge_dim,\n",
    "                                     beta=True)\n",
    "        self.transf2 = Linear(1 * n_heads, 1)\n",
    "        self.bn2 = BatchNorm1d(1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.transf1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.conv_layers[i](x, edge_index, edge_attr)\n",
    "            x = torch.relu(self.transf_layers[i](x))\n",
    "            # print(\"i\",i)\n",
    "\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.transf2(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "    ########################################################################\n",
    "    ##   How to change the code above in order to have a good model ?    ##\n",
    "    ########################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b925ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparametes \n",
    "params_model = {\"embedding_size\": 500,\n",
    "                \"n_heads\": 4,\n",
    "                \"dropout_rate\": 0.9,\n",
    "                \"edge_dim\": 2,\n",
    "                \"model_layers\": 4,\n",
    "                }\n",
    "\n",
    "model = Model01(feature_size=dataset[0].x.shape[1], model_params=params_model)\n",
    "print(\"Model:\", model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untrained model\n",
    "model = model.to(device)\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "prediction = model(data.x, data.edge_index, data.edge_attr)\n",
    "print(\"prediction:\", prediction)\n",
    "print(\"prediction shape:\", prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contraction_loss(predictions, graphs_info, print_path=False):\n",
    "    # path = predictions.reshape(predictions.shape[0])\n",
    "    # print(\"predictions\", predictions)\n",
    "    # print(\"predictions shape\", predictions.shape)\n",
    "    predictions = predictions.reshape((1, predictions.shape[0]))\n",
    "    contraction_cost_list = torch.tensor([[0.0]], dtype=torch.float32, requires_grad=True)\n",
    "    for predicted_path in predictions:\n",
    "        path = predicted_path\n",
    "        path = edge_path_to_opt_einsum_path(path, graphs_info[\"opt_einsum_input\"].copy())\n",
    "        if print_path:\n",
    "            print(\"\\npath in opt_einsum format:\", path)\n",
    "        contraction_cost = cost_of_contraction(path, graphs_info, importance=importance_weights)\n",
    "        contraction_cost_list = torch.cat([contraction_cost_list, torch.tensor([[contraction_cost]])], dim=1)\n",
    "\n",
    "    ls = torch.sum(contraction_cost_list)\n",
    "\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e57cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "print(\"###########Training##############\")\n",
    "nr_epochs = 1\n",
    "lr = 0.0005\n",
    "\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(nr_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(dataset):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        sample = data.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        model_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        prediction = model(sample.x, sample.edge_index, sample.edge_attr)\n",
    "        loss = contraction_loss(prediction, data.y)\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'\\n[{epoch + 1}] loss: {running_loss:.3f}')\n",
    "    loss_hist.append(running_loss)\n",
    "\n",
    "print('-Finished Training-\\n ')\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(' epochs')\n",
    "plt.ylabel(' loss')\n",
    "plt.title('loss history')\n",
    "plt.savefig(\"figures/loss_history.png\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ebe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom on the last part of training\n",
    "plt.plot(loss_hist[0:])\n",
    "plt.xlabel(' epochs')\n",
    "plt.ylabel(' loss')\n",
    "plt.title('loss history cut ')\n",
    "plt.savefig(\"figures/loss_history_cut.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d419570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model after training\n",
    "data = dataset[0].to(device)\n",
    "prediction = model(data.x, data.edge_index, data.edge_attr)\n",
    "reshape_prediction = prediction.reshape((1, prediction.shape[0]))\n",
    "print(\"prediction:\", reshape_prediction)\n",
    "loss = contraction_loss(prediction, data.y, print_path=True)\n",
    "print(\"loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdaade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9185f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
